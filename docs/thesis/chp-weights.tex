\chapter{Parameter learning}\label{chp:weights}

The objective of this chapter is to expose the ideas behind generative and discriminative gradient
descent for parameter learning of sum-product networks. We first show how to derive the SPN with
respect to its nods and weights so that we can find the gradient of the SPN wrt its parameters
(i.e.\ weights). This allows us to find the weight updates needed for gradient descent on SPNs. We
then describe how to perform generative stochastic gradient descent, and finally discriminative
gradient descent.

\section{Derivatives}

Let $S$ be an SPN\@. We first show how to find the derivative $\iddspn{S}{S_j}$, that is, the
derivative of the SPN $S$ with respect to some sub-SPN $S_j$. This derivative depends on whether
$j$ is a sum or product node. Let us assume that $j$ is a sum.
